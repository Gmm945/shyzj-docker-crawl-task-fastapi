# 爬虫镜像构建规范

## 概述

本文档定义了爬虫任务Docker镜像的构建规范，确保镜像符合数据采集任务管理系统的要求。

## 基础镜像要求

### 推荐基础镜像
```dockerfile
FROM python:3.9-slim
```

### 其他可选基础镜像
- `python:3.10-slim`
- `python:3.11-slim`
- `alpine:3.18` (需要额外安装Python)

## 镜像构建规范

### 1. 系统依赖安装

```dockerfile
# 安装必要的系统依赖
RUN apt-get update && apt-get install -y \
    gcc \
    libxml2-dev \
    libxslt1-dev \
    zlib1g-dev \
    curl \
    && rm -rf /var/lib/apt/lists/*
```

### 2. Python依赖管理

```dockerfile
# 复制requirements.txt
COPY requirements.txt /app/

# 安装Python依赖
RUN pip install --no-cache-dir -r requirements.txt
```

### 3. 应用代码部署

```dockerfile
# 复制应用代码
COPY crawler_service.py /app/

# 创建必要的目录
RUN mkdir -p /app/config /app/output
```

### 4. 环境变量设置

```dockerfile
# 设置环境变量
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1
```

### 5. 用户权限设置

```dockerfile
# 创建非root用户
RUN useradd -m -u 1000 crawler && \
    chown -R crawler:crawler /app

# 切换到非root用户
USER crawler
```

### 6. 健康检查

```dockerfile
# 健康检查（可选）
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health', timeout=5)" || exit 1
```

### 7. 启动命令

```dockerfile
# 运行爬虫服务
CMD ["python", "crawler_service.py"]
```

## 完整Dockerfile示例

```dockerfile
# 爬虫任务容器镜像构建文件
FROM python:3.9-slim

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    gcc \
    libxml2-dev \
    libxslt1-dev \
    zlib1g-dev \
    curl \
    && rm -rf /var/lib/apt/lists/*

# 设置工作目录
WORKDIR /app

# 复制Python依赖文件
COPY requirements.txt /app/

# 安装Python依赖
RUN pip install --no-cache-dir -r requirements.txt

# 复制爬虫服务代码
COPY crawler_service.py /app/

# 创建必要的目录
RUN mkdir -p /app/config /app/output

# 设置环境变量
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# 创建非root用户
RUN useradd -m -u 1000 crawler && \
    chown -R crawler:crawler /app

# 切换到非root用户
USER crawler

# 运行爬虫服务
CMD ["python", "crawler_service.py"]
```

## 环境变量规范

### 必需环境变量

| 变量名 | 说明 | 示例值 |
|--------|------|--------|
| `TASK_EXECUTION_ID` | 任务执行ID（UUID格式） | `550e8400-e29b-41d4-a716-446655440000` |
| `CONFIG_PATH` | 配置文件路径 | `/app/config/config.json` |

### 可选环境变量

| 变量名 | 说明 | 默认值 |
|--------|------|--------|
| `API_BASE_URL` | 主控系统API地址 | `http://localhost:8000` |
| `LOG_LEVEL` | 日志级别 | `INFO` |

## 文件结构规范

```
/app/
├── crawler_service.py    # 主程序文件
├── config/              # 配置目录（挂载）
├── output/              # 输出目录（挂载）
└── requirements.txt     # Python依赖
```

## 端口规范

- **不需要暴露任何端口**
- 容器只通过HTTP请求与主控系统通信
- 所有通信都是出站请求

## 卷挂载规范

### 必需挂载

```bash
# 配置文件挂载
-v /host/config.json:/app/config/config.json:ro

# 输出目录挂载
-v /host/output:/app/output
```

### 可选挂载

```bash
# 日志目录挂载
-v /host/logs:/app/logs

# 缓存目录挂载
-v /host/cache:/app/cache
```

## 信号处理规范

容器必须正确处理以下信号：

### SIGTERM
- 优雅停止爬虫任务
- 发送完成通知
- 清理资源

### SIGINT (Ctrl+C)
- 同SIGTERM处理

## 日志规范

### 日志格式
```
YYYY-MM-DD HH:MM:SS - logger_name - LEVEL - message
```

### 日志级别
- `INFO`: 一般信息
- `DEBUG`: 调试信息
- `WARNING`: 警告信息
- `ERROR`: 错误信息

### 标准输出
- 所有日志输出到标准输出
- 使用`PYTHONUNBUFFERED=1`确保实时输出

## 心跳规范

### 心跳频率
- 每30秒发送一次心跳
- 心跳包含详细的进度信息

### 心跳数据格式
```json
{
  "execution_id": "uuid",
  "container_id": "container-id",
  "status": "running|completed|failed",
  "progress": {
    "total_urls": 100,
    "crawled_urls": 45,
    "successful_urls": 42,
    "failed_urls": 3,
    "current_url": "https://example.com",
    "current_stage": "爬取中 (45/100)",
    "data_items": 1250,
    "percentage": 45.0,
    "runtime_seconds": 1800
  },
  "timestamp": 1640995200
}
```

## 完成通知规范

### 成功完成
```json
{
  "execution_id": "uuid",
  "container_id": "container-id",
  "success": true,
  "result_data": {
    "crawl_summary": {...},
    "data_items": [...],
    "total_data_count": 1250
  }
}
```

### 失败完成
```json
{
  "execution_id": "uuid",
  "container_id": "container-id",
  "success": false,
  "result_data": null,
  "error_message": "具体错误信息"
}
```

## 镜像标签规范

### 推荐标签格式
```
crawler-service:latest
crawler-service:v1.0.0
your-company/crawler-service:latest
```

### 版本标签
- `latest`: 最新版本
- `v1.0.0`: 语义化版本
- `20240101`: 日期版本

## 构建命令

### 基本构建
```bash
docker build -t crawler-service:latest .
```

### 带标签构建
```bash
docker build -t your-registry/crawler-service:v1.0.0 .
```

### 多架构构建
```bash
docker buildx build --platform linux/amd64,linux/arm64 -t crawler-service:latest .
```

## 测试规范

### 构建测试
```bash
# 构建镜像
docker build -t crawler-service:test .

# 运行测试
docker run --rm \
  -e TASK_EXECUTION_ID=test-uuid \
  -e CONFIG_PATH=/app/config/config.json \
  -v $(pwd)/config_examples/basic_crawler.json:/app/config/config.json:ro \
  crawler-service:test
```

### 功能测试
- 心跳发送正常
- 进度报告准确
- 优雅停止功能
- 错误处理机制

## 注意事项

1. **安全性**
   - 使用非root用户运行
   - 不暴露敏感信息到环境变量
   - 定期更新基础镜像

2. **性能**
   - 优化镜像大小
   - 使用多阶段构建（如需要）
   - 合理设置资源限制

3. **兼容性**
   - 确保Python版本兼容
   - 测试不同操作系统的兼容性
   - 验证依赖包的兼容性

4. **维护性**
   - 清晰的代码注释
   - 详细的日志输出
   - 易于调试的配置
