# 爬虫容器使用示例

## 概述

本文档提供了爬虫容器的完整使用示例，包括构建、运行、监控等各个环节。

## 快速开始

### 1. 构建镜像

```bash
# 进入demo目录
cd crawler_demo

# 构建镜像
docker build -t crawler-service:latest .
```

### 2. 准备配置文件

```bash
# 创建配置目录
mkdir -p /tmp/task_configs/demo-execution

# 复制配置文件
cp config_examples/basic_crawler.json /tmp/task_configs/demo-execution/config.json

# 创建输出目录
mkdir -p /tmp/crawler_outputs
```

### 3. 运行容器

```bash
docker run -d \
  --name crawler-demo \
  --rm \
  -v /tmp/task_configs/demo-execution/config.json:/app/config/config.json:ro \
  -v /tmp/crawler_outputs:/app/output \
  -e TASK_EXECUTION_ID=550e8400-e29b-41d4-a716-446655440000 \
  -e CONFIG_PATH=/app/config/config.json \
  -e API_BASE_URL=http://localhost:8000 \
  crawler-service:latest
```

### 4. 查看日志

```bash
# 查看容器日志
docker logs -f crawler-demo

# 查看输出文件
ls -la /tmp/crawler_outputs/
```

## 详细示例

### 示例1: 基础爬虫任务

#### 配置文件

**重要说明**: 配置文件中的 `docker_image` 字段用于指定第三方构建的爬虫容器镜像。镜像必须符合我们的构建规范（详见[镜像构建规范](镜像构建规范.md)）。
```json
{
  "task_name": "基础爬虫任务",
  "task_type": "docker-crawl",
  "docker_image": "your-company/crawler-service:latest",
  "base_url": "https://httpbin.org",
  "target_urls": [
    "https://httpbin.org/get",
    "https://httpbin.org/json",
    "https://httpbin.org/html"
  ],
  "user_agent": "Mozilla/5.0 (compatible; DataCollector/1.0)",
  "timeout": 30,
  "delay": 2
}
```

#### 运行命令
```bash
docker run --rm \
  -v $(pwd)/config_examples/basic_crawler.json:/app/config/config.json:ro \
  -v $(pwd)/output:/app/output \
  -e TASK_EXECUTION_ID=basic-test-$(date +%s) \
  -e CONFIG_PATH=/app/config/config.json \
  crawler-service:latest
```

### 示例2: 电商网站爬虫

#### 配置文件
```json
{
  "task_name": "电商网站爬虫",
  "task_type": "docker-crawl",
  "base_url": "https://httpbin.org",
  "target_urls": [
    "https://httpbin.org/html",
    "https://httpbin.org/json"
  ],
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
  "timeout": 45,
  "delay": 3,
  "max_retries": 5
}
```

#### 运行命令
```bash
docker run --rm \
  -v $(pwd)/config_examples/ecommerce_crawler.json:/app/config/config.json:ro \
  -v $(pwd)/output:/app/output \
  -e TASK_EXECUTION_ID=ecommerce-test-$(date +%s) \
  -e CONFIG_PATH=/app/config/config.json \
  crawler-service:latest
```

### 示例3: 新闻网站爬虫

#### 配置文件
```json
{
  "task_name": "新闻网站爬虫",
  "task_type": "docker-crawl",
  "base_url": "https://httpbin.org",
  "target_urls": [
    "https://httpbin.org/html",
    "https://httpbin.org/json"
  ],
  "user_agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
  "timeout": 60,
  "delay": 5,
  "max_retries": 3
}
```

#### 运行命令
```bash
docker run --rm \
  -v $(pwd)/config_examples/news_crawler.json:/app/config/config.json:ro \
  -v $(pwd)/output:/app/output \
  -e TASK_EXECUTION_ID=news-test-$(date +%s) \
  -e CONFIG_PATH=/app/config/config.json \
  crawler-service:latest
```

## 生产环境部署

### 1. 使用Docker Compose

```yaml
version: '3.8'

services:
  crawler-service:
    build: .
    container_name: crawler-task-${EXECUTION_ID}
    environment:
      - TASK_EXECUTION_ID=${EXECUTION_ID}
      - CONFIG_PATH=/app/config/config.json
      - API_BASE_URL=http://data-platform-api:8000
    volumes:
      - ./configs/${EXECUTION_ID}/config.json:/app/config/config.json:ro
      - ./outputs/${EXECUTION_ID}:/app/output
    restart: "no"
    networks:
      - crawler-network

networks:
  crawler-network:
    external: true
```

#### 运行Docker Compose
```bash
# 设置环境变量
export EXECUTION_ID=550e8400-e29b-41d4-a716-446655440000

# 创建配置目录
mkdir -p configs/${EXECUTION_ID}
cp config_examples/basic_crawler.json configs/${EXECUTION_ID}/config.json

# 创建输出目录
mkdir -p outputs/${EXECUTION_ID}

# 运行服务
docker-compose up
```

### 2. Kubernetes部署

#### Deployment配置
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: crawler-task
  labels:
    app: crawler-task
spec:
  replicas: 1
  selector:
    matchLabels:
      app: crawler-task
  template:
    metadata:
      labels:
        app: crawler-task
    spec:
      containers:
      - name: crawler-service
        image: crawler-service:latest
        env:
        - name: TASK_EXECUTION_ID
          value: "550e8400-e29b-41d4-a716-446655440000"
        - name: CONFIG_PATH
          value: "/app/config/config.json"
        - name: API_BASE_URL
          value: "http://data-platform-api:8000"
        volumeMounts:
        - name: config-volume
          mountPath: /app/config
          readOnly: true
        - name: output-volume
          mountPath: /app/output
      volumes:
      - name: config-volume
        configMap:
          name: crawler-config
      - name: output-volume
        persistentVolumeClaim:
          claimName: crawler-output-pvc
```

#### ConfigMap配置
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: crawler-config
data:
  config.json: |
    {
      "task_name": "Kubernetes爬虫任务",
      "task_type": "docker-crawl",
      "base_url": "https://httpbin.org",
      "target_urls": [
        "https://httpbin.org/html",
        "https://httpbin.org/json"
      ],
      "user_agent": "Mozilla/5.0 (compatible; DataCollector/1.0)",
      "timeout": 30,
      "delay": 2
    }
```

## 监控和调试

### 1. 查看容器状态

```bash
# 查看运行中的容器
docker ps

# 查看容器详细信息
docker inspect crawler-demo

# 查看容器资源使用
docker stats crawler-demo
```

### 2. 查看日志

```bash
# 实时查看日志
docker logs -f crawler-demo

# 查看最近的日志
docker logs --tail 100 crawler-demo

# 查看特定时间段的日志
docker logs --since="2024-01-01T10:00:00" crawler-demo
```

### 3. 进入容器调试

```bash
# 进入运行中的容器
docker exec -it crawler-demo /bin/bash

# 查看配置文件
cat /app/config/config.json

# 查看输出文件
ls -la /app/output/

# 查看环境变量
env | grep -E "(TASK_EXECUTION_ID|CONFIG_PATH|API_BASE_URL)"
```

### 4. 心跳监控

```bash
# 模拟心跳请求
curl -X POST http://localhost:8000/api/v1/monitoring/heartbeat \
  -H "Content-Type: application/json" \
  -d '{
    "execution_id": "550e8400-e29b-41d4-a716-446655440000",
    "container_id": "crawler-demo",
    "status": "running",
    "progress": {
      "total_urls": 10,
      "crawled_urls": 5,
      "successful_urls": 5,
      "failed_urls": 0,
      "percentage": 50.0
    },
    "timestamp": '$(date +%s)'
  }'
```

## 故障排除

### 1. 容器启动失败

#### 检查镜像
```bash
# 查看镜像是否存在
docker images | grep crawler-service

# 重新构建镜像
docker build -t crawler-service:latest .
```

#### 检查配置文件
```bash
# 检查配置文件格式
cat /tmp/task_configs/demo-execution/config.json | jq .

# 验证JSON格式
python -m json.tool /tmp/task_configs/demo-execution/config.json
```

#### 检查环境变量
```bash
# 检查必需的环境变量
docker run --rm crawler-service:latest env | grep -E "(TASK_EXECUTION_ID|CONFIG_PATH)"
```

### 2. 心跳发送失败

#### 检查网络连接
```bash
# 测试API连接
docker run --rm --network host curlimages/curl \
  curl -v http://localhost:8000/api/v1/monitoring/heartbeat
```

#### 检查API服务
```bash
# 检查API服务状态
curl http://localhost:8000/health

# 检查心跳接口
curl -X POST http://localhost:8000/api/v1/monitoring/heartbeat \
  -H "Content-Type: application/json" \
  -d '{"execution_id":"test","container_id":"test","timestamp":1640995200}'
```

### 3. 爬虫任务异常

#### 检查目标网站
```bash
# 测试目标网站访问
curl -v https://httpbin.org/html

# 检查网络延迟
ping -c 4 httpbin.org
```

#### 检查爬虫逻辑
```bash
# 进入容器查看日志
docker exec -it crawler-demo tail -f /dev/stdout

# 查看错误日志
docker logs crawler-demo 2>&1 | grep ERROR
```

## 性能优化

### 1. 资源限制

```bash
# 限制CPU和内存使用
docker run --rm \
  --cpus="1.0" \
  --memory="512m" \
  --memory-swap="1g" \
  crawler-service:latest
```

### 2. 并发控制

```json
{
  "task_name": "高性能爬虫",
  "concurrency": {
    "max_workers": 5,
    "delay_between_requests": 1,
    "batch_size": 100
  }
}
```

### 3. 缓存优化

```bash
# 使用本地缓存
docker run --rm \
  -v crawler-cache:/app/cache \
  crawler-service:latest
```

## 安全考虑

### 1. 网络安全

```bash
# 使用自定义网络
docker network create crawler-network

docker run --rm \
  --network crawler-network \
  crawler-service:latest
```

### 2. 数据安全

```bash
# 加密敏感数据
docker run --rm \
  -e CONFIG_ENCRYPTION_KEY=your-encryption-key \
  crawler-service:latest
```

### 3. 权限控制

```bash
# 使用非特权用户
docker run --rm \
  --user 1000:1000 \
  crawler-service:latest
```

## 扩展开发

### 1. 自定义爬虫逻辑

```python
# 继承SimpleCrawler类
class CustomCrawler(SimpleCrawler):
    def _crawl_single_url(self, url: str) -> bool:
        # 自定义爬虫逻辑
        pass
    
    def _parse_page_content(self, content: str, url: str) -> List[Dict]:
        # 自定义解析逻辑
        pass
```

### 2. 添加新的配置选项

```json
{
  "task_name": "扩展爬虫",
  "custom_options": {
    "use_proxy": true,
    "proxy_list": ["proxy1:8080", "proxy2:8080"],
    "retry_strategy": "exponential_backoff"
  }
}
```

### 3. 集成外部服务

```python
# 集成数据库
from sqlalchemy import create_engine

class DatabaseCrawler(SimpleCrawler):
    def __init__(self, config):
        super().__init__(config)
        self.db_engine = create_engine(config['database_url'])
    
    def save_data(self, data):
        # 保存到数据库
        pass
```
